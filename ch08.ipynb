{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python访问网页和HTTP协议\n",
    "## 一、Web和HTTP协议\n",
    "- 1990年，万维网WWW的发明者蒂姆·伯纳斯·李，基于HTTP协议实现与服务器的第一次通讯。\n",
    "- 1994年，网景公司发布 Mosaic Netscape 1.0 beta 浏览器，后改名为Netscape Navigator。\n",
    "\n",
    "![](images/web.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. HTTP协议基础\n",
    "HTTP（超文本传输协议）是基于 TCP/IP 协议的应用层协议。它不涉及数据包传输，主要规定了客户端和服务器之间的通信格式，默认使用80端口。\n",
    "HTTPS（超文本传输安全协议）经由HTTPS协议进行通信，但是利用SSL/TLS加密数据包，默认使用443端口。\n",
    "#### URL\n",
    "URL(统一资源定位器)，用来唯一标识网络资源。例如：http://quote.eastmoney.com/sh600019.html\n",
    "#### HTTP请求和响应\n",
    "- HTTP请求由三部分组成，分别是：请求行、消息报头、请求正文。常见的请求方式\n",
    "    - get\n",
    "    - post\n",
    "- HTTP响应由三部分组成，分别是：状态行、消息报头、响应正文。常见的状态码有\n",
    "    - 200 OK\n",
    "    - 400 Bad Request\n",
    "    - 401 Unauthorized\n",
    "    - 403 Forbidden\n",
    "    - 404 Not Found\n",
    "    - 500 Internal Server Error\n",
    "    - 503 Server Unavailable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. HTML、CSS和JavaScirpt\n",
    "HTTP协议响应正文主要是：文本内容，如HTML、CSS和JavaScirpt文件，也可以是任意文本类型内容，如XML和JSON文件等；二进制内容，如图片和其他任意二级制文件。\n",
    "\n",
    "![](images/web.png)\n",
    "\n",
    "其中HTML、CSS和JavaScirpt是构建Web页面的核心内容和技术\n",
    "\n",
    "![](images/html.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、Python访问网页"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 使用urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.baidu.com 200 OK\n",
      "<html>\n",
      "<head>\n",
      "\t<script>\n",
      "\t\tlocation.replace(location.href.replace(\"https://\",\"http://\"));\n",
      "\t</script>\n",
      "</head>\n",
      "<body>\n",
      "\t<noscript><meta http-equiv=\"refresh\" content=\"0;url=http://www.baidu.com/\"></noscript>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from urllib import request # python 2中是urllib2\n",
    "resp = request.urlopen('https://www.baidu.com')\n",
    "print(resp.geturl(), resp.status, resp.reason, sep=' ')\n",
    "print(resp.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200 OK\n"
     ]
    }
   ],
   "source": [
    "resp = request.urlopen('http://quote.eastmoney.com/newapi/getlrqs?code=600019')\n",
    "print('Status:', resp.status, resp.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': 'Thu, 03 Jun 2021 13:56:31 GMT',\n",
       " 'Content-Type': 'application/json; charset=utf-8',\n",
       " 'Content-Length': '657',\n",
       " 'Connection': 'close',\n",
       " 'Server': 'Tengine',\n",
       " 'Age': '1',\n",
       " 'X-Via': '1.1 PS-WNZ-01c1W35:12 (Cdn Cache Server V2.0), 1.1 PS-FOC-01SoY26:24 (Cdn Cache Server V2.0)',\n",
       " 'X-Ws-Request-Id': '60b8df8f_oudxin23_26989-20527'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in resp.getheaders()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'code': '600019', 'Profit': 2725832313.54, 'RptDate': '2019-03-31T00:00:00'},\n",
       " {'code': '600019', 'Profit': 3613532099.7, 'RptDate': '2019-06-30T00:00:00'},\n",
       " {'code': '600019', 'Profit': 2679718306.09, 'RptDate': '2019-09-30T00:00:00'},\n",
       " {'code': '600019', 'Profit': 3543935484.89, 'RptDate': '2019-12-31T00:00:00'},\n",
       " {'code': '600019', 'Profit': 1540624304.54, 'RptDate': '2020-03-31T00:00:00'},\n",
       " {'code': '600019', 'Profit': 2461743613.88, 'RptDate': '2020-06-30T00:00:00'},\n",
       " {'code': '600019', 'Profit': 3856596954.59, 'RptDate': '2020-09-30T00:00:00'},\n",
       " {'code': '600019', 'Profit': 4817810871.38, 'RptDate': '2020-12-31T00:00:00'},\n",
       " {'code': '600019', 'Profit': 5359008715.48, 'RptDate': '2021-03-31T00:00:00'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.loads(resp.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 使用requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.baidu.com/ 200 OK\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "resp = requests.get('https://www.baidu.com')\n",
    "print(resp.url, resp.status_code, resp.reason, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200 OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'code': '600019', 'Profit': 2725832313.54, 'RptDate': '2019-03-31T00:00:00'},\n",
       " {'code': '600019', 'Profit': 3613532099.7, 'RptDate': '2019-06-30T00:00:00'},\n",
       " {'code': '600019', 'Profit': 2679718306.09, 'RptDate': '2019-09-30T00:00:00'},\n",
       " {'code': '600019', 'Profit': 3543935484.89, 'RptDate': '2019-12-31T00:00:00'},\n",
       " {'code': '600019', 'Profit': 1540624304.54, 'RptDate': '2020-03-31T00:00:00'},\n",
       " {'code': '600019', 'Profit': 2461743613.88, 'RptDate': '2020-06-30T00:00:00'},\n",
       " {'code': '600019', 'Profit': 3856596954.59, 'RptDate': '2020-09-30T00:00:00'},\n",
       " {'code': '600019', 'Profit': 4817810871.38, 'RptDate': '2020-12-31T00:00:00'},\n",
       " {'code': '600019', 'Profit': 5359008715.48, 'RptDate': '2021-03-31T00:00:00'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "resp = requests.get('http://quote.eastmoney.com/newapi/getlrqs?code=600019')\n",
    "print('Status:', resp.status_code, resp.reason)\n",
    "json.loads(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 使用pyspider和scrapy\n",
    "pyspider是国人开发的爬虫框架，支持多线程，js动态解析，带有web操作界面，支持多种数据库。\n",
    "http://docs.pyspider.org/\n",
    "\n",
    "scrapy \n",
    "https://scrapy-chs.readthedocs.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python\n",
    "# -*- encoding: utf-8 -*-\n",
    "# Created on 2021-05-01 12:03:36\n",
    "# Project: test\n",
    "\n",
    "from pyspider.libs.base_handler import *\n",
    "\n",
    "class Handler(BaseHandler):\n",
    "    crawl_config = {\n",
    "    }\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.start_url = 'https://www.baidu.com'\n",
    "        self.request_headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.96 Safari/537.36'\n",
    "        }         \n",
    "\n",
    "    @every(minutes=24 * 60)\n",
    "    def on_start(self):  \n",
    "        save = {'id': 0}\n",
    "        self.crawl(self.start_url, callback=self.index_page, headers=self.request_headers, save=save, retries=3)\n",
    "\n",
    "    @config(age=10 * 24 * 60 * 60)\n",
    "    def index_page(self, response):        \n",
    "        response.save['id'] += 1 \n",
    "        items = response.doc('a[href^=\"http\"]').items()\n",
    "        for item in items:\n",
    "            self.crawl(item.attr.href, callback=self.detail_page, save=response.save, retries=3)\n",
    "        return {'id': response.save['id'], 'url': response.url, 'status_code': response.status_code, \n",
    "                'title': response.doc('title').text()}\n",
    "\n",
    "    @config(priority=2)\n",
    "    def detail_page(self, response):        \n",
    "        response.save['id'] += 1\n",
    "        return {'id': response.save['id'], 'url': response.url, 'status_code': response.status_code, \n",
    "                'title': response.doc('title').text()}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 使用pyquery、beautifullsoap\n",
    "- pyquery https://pythonhosted.org/pyquery/api.html\n",
    "- beautifullsoap https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html\n",
    "- xpath https://www.w3school.com.cn/xpath/xpath_syntax.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 编程实践：尝试获东方财富、新浪财经上的金融数据页面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、代码阅读：基于requests的小型爬虫框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "from typing import Mapping, Dict, List, Callable, Optional, Any\n",
    "import requests\n",
    "from requests import Response\n",
    "import pandas as pd\n",
    "import logging\n",
    "from pyquery import PyQuery\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(thread)d %(levelname)s %(module)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def retry(num: int):\n",
    "    \"\"\"\n",
    "    重试装饰器，若被装饰函数执行失败，则重试num次\n",
    "    :param num: 重试次数\n",
    "    \"\"\"\n",
    "\n",
    "    def retry_decorator(func: Callable):\n",
    "        @wraps(func)\n",
    "        def retry_wrapper(*args, **kwargs):\n",
    "            for i in range(num + 1):\n",
    "                if i > 0:\n",
    "                    logger.info('Retry: %d/%d' % (i, num))\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    logger.error('Error: %s' % e, exc_info=True)\n",
    "                    time.sleep(1)\n",
    "\n",
    "        return retry_wrapper\n",
    "\n",
    "    return retry_decorator\n",
    "\n",
    "\n",
    "class CrawlerBase:\n",
    "    \"\"\"\n",
    "    爬虫框架基类\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.request_headers: Dict = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.96 Safari/537.36'\n",
    "        }\n",
    "        self.results: Optional[List] = []\n",
    "        self.methods: Mapping[str, Callable] = {\n",
    "            'get': self.get,\n",
    "            'post': self.post\n",
    "        }\n",
    "        self.jobs = []\n",
    "\n",
    "    @retry(3)\n",
    "    def get(self, url: str, headers: Optional[Dict] = None, *args, **kwargs) -> Response:\n",
    "        \"\"\"\n",
    "        GET请求\n",
    "        :param url: 请求URL\n",
    "        :param headers: 请求头\n",
    "        :param args: requests请求位置参数\n",
    "        :param kwargs: requests请求关键字参数\n",
    "        :return: Response 响应对象\n",
    "        \"\"\"\n",
    "        _headers: Dict = headers.copy() if headers else {}\n",
    "        _headers.update(self.request_headers)\n",
    "        response: Response = requests.get(url, headers=_headers, *args, **kwargs)\n",
    "        logger.info('[GET %d] %s' % (response.status_code, response.url))\n",
    "        return response\n",
    "\n",
    "    @retry(3)\n",
    "    def post(self, url: str, data: Optional[Any] = None, headers: Optional[Dict] = None, *args, **kwargs) -> Response:\n",
    "        \"\"\"\n",
    "        POST请求\n",
    "        :param url: 请求URL\n",
    "        :param data: 请求BODY\n",
    "        :param headers: 请求头\n",
    "        :param args: requests请求位置参数\n",
    "        :param kwargs: requests请求关键字参数\n",
    "        :return: 响应对象\n",
    "        \"\"\"\n",
    "        _headers: Dict = headers.copy() if headers else {}\n",
    "        _headers.update(self.request_headers)\n",
    "        response: Response = requests.post(url, data=data, headers=_headers, *args, **kwargs)\n",
    "        logger.info('[POST %d] %s' % (response.status_code, response.url))\n",
    "        return response\n",
    "\n",
    "    def crawl(self, url: str, callback: Callable, save: Optional[Mapping] = None,\n",
    "              method: Optional[str] = 'get', wait: int = 0, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        发起一个请求，并用回调函数处理结果\n",
    "        :param url: 请求URL\n",
    "        :param callback: 回调函数\n",
    "        :param save: 向回调函数传递数据\n",
    "        :param method: 请求方式，支持get，post，默认为get\n",
    "        :param wait: 等待时长，单位秒\n",
    "        :param args: requests请求位置参数\n",
    "        :param kwargs: requests请求关键字参数\n",
    "        \"\"\"\n",
    "\n",
    "        if method not in self.methods.keys():\n",
    "            raise 'Method not allowed: %s' % method\n",
    "        try:\n",
    "            if url not in self.jobs:\n",
    "                time.sleep(wait)\n",
    "                response = self.methods[method](url, *args, **kwargs)\n",
    "                self.jobs.append(url)\n",
    "                callback(response, save=save)\n",
    "        except Exception as e:\n",
    "            logger.error('Crawl [%s] error' % url, exc_info=True)\n",
    "\n",
    "    def save_data(self, record: Any) -> None:\n",
    "        \"\"\"\n",
    "        在内存中缓存单条记录\n",
    "        :param record: 单条记录对象\n",
    "        \"\"\"\n",
    "        if self.results is None:\n",
    "            self.results = []\n",
    "        self.results.append(record)\n",
    "        logger.debug('Saved: %s' % record)\n",
    "\n",
    "    def to_excel(self, filename: Optional[str] = 'data', sheet_name: Optional[str] = 'data') -> None:\n",
    "        \"\"\"\n",
    "        将全部缓存的数据导出为Excel\n",
    "        :param filename 文件名，不包含后缀名\n",
    "        :param sheet_name excel 表名\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result_df = pd.DataFrame.from_records(data=self.results)\n",
    "            result_df.to_excel(\"%s.xlsx\" % filename, sheet_name=sheet_name, index=False)\n",
    "            logger.info('Save to excel successfully')\n",
    "        except Exception as e:\n",
    "            logger.error('Save to excel failed', exc_info=True)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 获取百度首页下所有链接页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCrawler(CrawlerBase):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.start_url = 'https://www.baidu.com'\n",
    "\n",
    "    def run(self):\n",
    "        save = {'id': 0}\n",
    "        self.crawl(self.start_url, self.index_page, save=save, method='get')\n",
    "\n",
    "    def index_page(self, response: Response, save: Optional[Mapping] = None):\n",
    "        doc = PyQuery(response.text)\n",
    "        save['id'] += 1\n",
    "        self.save_data(\n",
    "            {'id': save['id'], 'url': response.url, 'status_code': response.status_code, 'title': doc('title').text()})\n",
    "        items = doc('a[href^=\"http\"]').items()\n",
    "        for item in items:\n",
    "            self.crawl(item.attr.href, self.detail_page, save=save)\n",
    "\n",
    "    def detail_page(self, response: Response, save: Optional[Mapping] = None):\n",
    "        doc = PyQuery(response.text)\n",
    "        save['id'] += 1\n",
    "        self.save_data(\n",
    "            {'id': save['id'], 'url': response.url, 'status_code': response.status_code, 'title': doc('title').text()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = TestCrawler()\n",
    "c.run()\n",
    "print(c.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 获取百度搜索结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchCrawler(CrawlerBase):\n",
    "\n",
    "    def __init__(self, keys):\n",
    "        super().__init__()\n",
    "        self.keys = keys\n",
    "        self.start_url = 'https://www.baidu.com/s?wd=%s%%20site:open.163.com'\n",
    "        self.flv_url = 'http://www.flvcd.com/parse.php?format=&kw=%s'\n",
    "        self.open_url = 'https://open.163.com/newview/movie/free?pid=%s'\n",
    "        self.request_headers['Accept'] = '*/*'\n",
    "        self.request_headers['Accept-Encoding'] = 'gzip, deflate, br'\n",
    "        self.request_headers['Accept-Language'] = 'zh-CN,zh;q=0.9,en;q=0.8'\n",
    "\n",
    "    def run(self):\n",
    "        for key in self.keys:\n",
    "            self.crawl(self.start_url % key, self.search_page, method='get', wait=3)\n",
    "\n",
    "    def search_page(self, response: Response, save: Optional[Mapping] = None):\n",
    "        logger.info('Get search page: %d %s', response.status_code, response.url)\n",
    "        doc = PyQuery(response.text)\n",
    "        items = [(item('a').attr('href'), item('div.c-span-last').text()) for item in\n",
    "                 doc('div.c-container div.c-gap-top-small').items()]\n",
    "        if len(items) > 0:\n",
    "            self.crawl(items[0][0], self.redirect_page, method='get', allow_redirects=False, wait=3)\n",
    "\n",
    "    def redirect_page(self, response: Response, save: Optional[Mapping] = None):\n",
    "        logger.info('Get redirect page: %d %s', response.status_code, response.url)\n",
    "        redirect_url = ''\n",
    "        if response.status_code == 302:\n",
    "            redirect_url = response.headers.get('location')\n",
    "        elif response.status_code == 200:\n",
    "            match = re.search(r'URL=\\'(.*?)\\'', response.text, re.S)\n",
    "            redirect_url = match.group(1)\n",
    "        if redirect_url:\n",
    "            self.crawl(self.flv_url % redirect_url, self.video_page, method='get')\n",
    "\n",
    "    def video_page(self, response: Response, save: Optional[Mapping] = None):\n",
    "        logger.info('Get video page: %d %s', response.status_code, response.url)\n",
    "        doc = PyQuery(response.text)\n",
    "        items = [item.attr('href') for item in doc('a[href^=\"http\"].link').items()]\n",
    "        if len(items) > 0 and items[0]:\n",
    "            video_url = items[0]\n",
    "            self.save_data(video_url)\n",
    "            logger.info('Finally video url is: %s', video_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-04 08:58:12,253 4502040064 DEBUG connectionpool - Starting new HTTPS connection (1): www.baidu.com:443\n",
      "2021-06-04 08:58:12,437 4502040064 DEBUG connectionpool - https://www.baidu.com:443 \"GET /s?wd=%E5%A6%82%E4%BD%95%E6%8E%8C%E6%8E%A7%E4%BD%A0%E7%9A%84%E8%87%AA%E7%94%B1%E6%97%B6%E9%97%B4%20site:open.163.com HTTP/1.1\" 200 None\n",
      "2021-06-04 08:58:13,143 4502040064 INFO <ipython-input-19-4da83ac8ec60> - [GET 200] https://www.baidu.com/s?wd=%E5%A6%82%E4%BD%95%E6%8E%8C%E6%8E%A7%E4%BD%A0%E7%9A%84%E8%87%AA%E7%94%B1%E6%97%B6%E9%97%B4%20site:open.163.com\n",
      "2021-06-04 08:58:13,145 4502040064 INFO <ipython-input-21-71bd470c92a0> - Get search page: 200 https://www.baidu.com/s?wd=%E5%A6%82%E4%BD%95%E6%8E%8C%E6%8E%A7%E4%BD%A0%E7%9A%84%E8%87%AA%E7%94%B1%E6%97%B6%E9%97%B4%20site:open.163.com\n",
      "2021-06-04 08:58:16,185 4502040064 DEBUG connectionpool - Starting new HTTP connection (1): www.baidu.com:80\n",
      "2021-06-04 08:58:16,233 4502040064 DEBUG connectionpool - http://www.baidu.com:80 \"GET /link?url=GMi5DwdEVO5HvUE0bZrSgD4nWrtJna-TJIF1wz5XwkYyoiJiJTI0rbR1K41cV_8f8PaM_8DLYr0q32J6gwYKWeeoJSXWT6XVtBoaTI_DZmC HTTP/1.1\" 302 154\n",
      "2021-06-04 08:58:16,236 4502040064 INFO <ipython-input-19-4da83ac8ec60> - [GET 302] http://www.baidu.com/link?url=GMi5DwdEVO5HvUE0bZrSgD4nWrtJna-TJIF1wz5XwkYyoiJiJTI0rbR1K41cV_8f8PaM_8DLYr0q32J6gwYKWeeoJSXWT6XVtBoaTI_DZmC\n",
      "2021-06-04 08:58:16,237 4502040064 INFO <ipython-input-21-71bd470c92a0> - Get redirect page: 302 http://www.baidu.com/link?url=GMi5DwdEVO5HvUE0bZrSgD4nWrtJna-TJIF1wz5XwkYyoiJiJTI0rbR1K41cV_8f8PaM_8DLYr0q32J6gwYKWeeoJSXWT6XVtBoaTI_DZmC\n",
      "2021-06-04 08:58:16,240 4502040064 DEBUG connectionpool - Starting new HTTP connection (1): www.flvcd.com:80\n",
      "2021-06-04 08:58:16,446 4502040064 DEBUG connectionpool - http://www.flvcd.com:80 \"GET /parse.php?format=&kw=https://open.163.com/newview/movie/courseintro?newurl=MC82BCQAN HTTP/1.1\" 302 20\n",
      "2021-06-04 08:58:16,451 4502040064 DEBUG connectionpool - Starting new HTTPS connection (1): www.flvcd.com:443\n",
      "2021-06-04 08:58:17,435 4502040064 DEBUG connectionpool - https://www.flvcd.com:443 \"GET /parse.php?format=&kw=https://open.163.com/newview/movie/courseintro?newurl=MC82BCQAN HTTP/1.1\" 200 None\n",
      "2021-06-04 08:58:17,441 4502040064 INFO <ipython-input-19-4da83ac8ec60> - [GET 200] https://www.flvcd.com/parse.php?format=&kw=https://open.163.com/newview/movie/courseintro?newurl=MC82BCQAN\n",
      "2021-06-04 08:58:17,442 4502040064 INFO <ipython-input-21-71bd470c92a0> - Get video page: 200 https://www.flvcd.com/parse.php?format=&kw=https://open.163.com/newview/movie/courseintro?newurl=MC82BCQAN\n",
      "2021-06-04 08:58:17,448 4502040064 DEBUG <ipython-input-19-4da83ac8ec60> - Saved: http://mov.bn.netease.com/open-movie/nos/mp4/2017/01/03/SC8U8K7BC_sd.mp4\n",
      "2021-06-04 08:58:17,450 4502040064 INFO <ipython-input-21-71bd470c92a0> - Finally video url is: http://mov.bn.netease.com/open-movie/nos/mp4/2017/01/03/SC8U8K7BC_sd.mp4\n",
      "2021-06-04 08:58:20,459 4502040064 DEBUG connectionpool - Starting new HTTPS connection (1): www.baidu.com:443\n",
      "2021-06-04 08:58:20,592 4502040064 DEBUG connectionpool - https://www.baidu.com:443 \"GET /s?wd=%E9%98%85%E8%AF%BB%E5%85%A8%E4%B8%96%E7%95%8C%20site:open.163.com HTTP/1.1\" 200 None\n",
      "2021-06-04 08:58:21,131 4502040064 INFO <ipython-input-19-4da83ac8ec60> - [GET 200] https://www.baidu.com/s?wd=%E9%98%85%E8%AF%BB%E5%85%A8%E4%B8%96%E7%95%8C%20site:open.163.com\n",
      "2021-06-04 08:58:21,132 4502040064 INFO <ipython-input-21-71bd470c92a0> - Get search page: 200 https://www.baidu.com/s?wd=%E9%98%85%E8%AF%BB%E5%85%A8%E4%B8%96%E7%95%8C%20site:open.163.com\n",
      "2021-06-04 08:58:24,157 4502040064 DEBUG connectionpool - Starting new HTTP connection (1): www.baidu.com:80\n",
      "2021-06-04 08:58:24,200 4502040064 DEBUG connectionpool - http://www.baidu.com:80 \"GET /link?url=Ng5I4VHWii1HGgTXbVeh7UNB6ejPiN2FaTQIf5i9wmL30QbLyKcnrQ-oiad_7roEBdcAQ0p4Jjd98sVmB93rhkFQmtatT-VzlpJoaKmN_HO HTTP/1.1\" 302 154\n",
      "2021-06-04 08:58:24,203 4502040064 INFO <ipython-input-19-4da83ac8ec60> - [GET 302] http://www.baidu.com/link?url=Ng5I4VHWii1HGgTXbVeh7UNB6ejPiN2FaTQIf5i9wmL30QbLyKcnrQ-oiad_7roEBdcAQ0p4Jjd98sVmB93rhkFQmtatT-VzlpJoaKmN_HO\n",
      "2021-06-04 08:58:24,204 4502040064 INFO <ipython-input-21-71bd470c92a0> - Get redirect page: 302 http://www.baidu.com/link?url=Ng5I4VHWii1HGgTXbVeh7UNB6ejPiN2FaTQIf5i9wmL30QbLyKcnrQ-oiad_7roEBdcAQ0p4Jjd98sVmB93rhkFQmtatT-VzlpJoaKmN_HO\n",
      "2021-06-04 08:58:24,207 4502040064 DEBUG connectionpool - Starting new HTTP connection (1): www.flvcd.com:80\n",
      "2021-06-04 08:58:24,312 4502040064 DEBUG connectionpool - http://www.flvcd.com:80 \"GET /parse.php?format=&kw=https://open.163.com/newview/movie/courseintro?newurl=MB9D1Q8U1 HTTP/1.1\" 302 20\n",
      "2021-06-04 08:58:24,318 4502040064 DEBUG connectionpool - Starting new HTTPS connection (1): www.flvcd.com:443\n",
      "2021-06-04 08:58:25,420 4502040064 DEBUG connectionpool - https://www.flvcd.com:443 \"GET /parse.php?format=&kw=https://open.163.com/newview/movie/courseintro?newurl=MB9D1Q8U1 HTTP/1.1\" 200 None\n",
      "2021-06-04 08:58:25,429 4502040064 INFO <ipython-input-19-4da83ac8ec60> - [GET 200] https://www.flvcd.com/parse.php?format=&kw=https://open.163.com/newview/movie/courseintro?newurl=MB9D1Q8U1\n",
      "2021-06-04 08:58:25,431 4502040064 INFO <ipython-input-21-71bd470c92a0> - Get video page: 200 https://www.flvcd.com/parse.php?format=&kw=https://open.163.com/newview/movie/courseintro?newurl=MB9D1Q8U1\n",
      "2021-06-04 08:58:25,436 4502040064 DEBUG <ipython-input-19-4da83ac8ec60> - Saved: http://mov.bn.netease.com/open-movie/nos/mp4/2015/12/08/SB9D26LE6_sd.mp4\n",
      "2021-06-04 08:58:25,436 4502040064 INFO <ipython-input-21-71bd470c92a0> - Finally video url is: http://mov.bn.netease.com/open-movie/nos/mp4/2015/12/08/SB9D26LE6_sd.mp4\n",
      "2021-06-04 08:58:28,443 4502040064 DEBUG connectionpool - Starting new HTTPS connection (1): www.baidu.com:443\n",
      "2021-06-04 08:58:28,599 4502040064 DEBUG connectionpool - https://www.baidu.com:443 \"GET /s?wd=%E5%A6%82%E4%BD%95%E5%81%9A%E5%BE%97%E6%9B%B4%E5%A5%BD%20site:open.163.com HTTP/1.1\" 200 None\n",
      "2021-06-04 08:58:29,143 4502040064 INFO <ipython-input-19-4da83ac8ec60> - [GET 200] https://www.baidu.com/s?wd=%E5%A6%82%E4%BD%95%E5%81%9A%E5%BE%97%E6%9B%B4%E5%A5%BD%20site:open.163.com\n",
      "2021-06-04 08:58:29,144 4502040064 INFO <ipython-input-21-71bd470c92a0> - Get search page: 200 https://www.baidu.com/s?wd=%E5%A6%82%E4%BD%95%E5%81%9A%E5%BE%97%E6%9B%B4%E5%A5%BD%20site:open.163.com\n",
      "2021-06-04 08:58:32,183 4502040064 DEBUG connectionpool - Starting new HTTP connection (1): www.baidu.com:80\n",
      "2021-06-04 08:58:32,234 4502040064 DEBUG connectionpool - http://www.baidu.com:80 \"GET /link?url=YxDzZyxJCW__x2Ynsn1W3s4_zjAYzCcni5WDgIYHWTWyYQ-4fxSFb9TWme10mvz-GQsvWsYwc5GC9bdTgE0cSpoF8__wFPnSUh9_e6PZuFW HTTP/1.1\" 302 154\n",
      "2021-06-04 08:58:32,237 4502040064 INFO <ipython-input-19-4da83ac8ec60> - [GET 302] http://www.baidu.com/link?url=YxDzZyxJCW__x2Ynsn1W3s4_zjAYzCcni5WDgIYHWTWyYQ-4fxSFb9TWme10mvz-GQsvWsYwc5GC9bdTgE0cSpoF8__wFPnSUh9_e6PZuFW\n",
      "2021-06-04 08:58:32,238 4502040064 INFO <ipython-input-21-71bd470c92a0> - Get redirect page: 302 http://www.baidu.com/link?url=YxDzZyxJCW__x2Ynsn1W3s4_zjAYzCcni5WDgIYHWTWyYQ-4fxSFb9TWme10mvz-GQsvWsYwc5GC9bdTgE0cSpoF8__wFPnSUh9_e6PZuFW\n",
      "2021-06-04 08:58:32,241 4502040064 DEBUG connectionpool - Starting new HTTP connection (1): www.flvcd.com:80\n",
      "2021-06-04 08:58:32,336 4502040064 DEBUG connectionpool - http://www.flvcd.com:80 \"GET /parse.php?format=&kw=https://open.163.com/movie/2017/2/C/U/MCC01J0QP_MCC021ACU.html HTTP/1.1\" 302 20\n",
      "2021-06-04 08:58:32,342 4502040064 DEBUG connectionpool - Starting new HTTPS connection (1): www.flvcd.com:443\n",
      "2021-06-04 08:58:32,896 4502040064 DEBUG connectionpool - https://www.flvcd.com:443 \"GET /parse.php?format=&kw=https://open.163.com/movie/2017/2/C/U/MCC01J0QP_MCC021ACU.html HTTP/1.1\" 200 4439\n",
      "2021-06-04 08:58:32,901 4502040064 INFO <ipython-input-19-4da83ac8ec60> - [GET 200] https://www.flvcd.com/parse.php?format=&kw=https://open.163.com/movie/2017/2/C/U/MCC01J0QP_MCC021ACU.html\n",
      "2021-06-04 08:58:32,902 4502040064 INFO <ipython-input-21-71bd470c92a0> - Get video page: 200 https://www.flvcd.com/parse.php?format=&kw=https://open.163.com/movie/2017/2/C/U/MCC01J0QP_MCC021ACU.html\n",
      "2021-06-04 08:58:32,906 4502040064 DEBUG <ipython-input-19-4da83ac8ec60> - Saved: http://mov.bn.netease.com/open-movie/nos/mp4/2017/02/10/SCC01VIDC_sd.mp4\n",
      "2021-06-04 08:58:32,907 4502040064 INFO <ipython-input-21-71bd470c92a0> - Finally video url is: http://mov.bn.netease.com/open-movie/nos/mp4/2017/02/10/SCC01VIDC_sd.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://mov.bn.netease.com/open-movie/nos/mp4/2017/01/03/SC8U8K7BC_sd.mp4', 'http://mov.bn.netease.com/open-movie/nos/mp4/2015/12/08/SB9D26LE6_sd.mp4', 'http://mov.bn.netease.com/open-movie/nos/mp4/2017/02/10/SCC01VIDC_sd.mp4']\n"
     ]
    }
   ],
   "source": [
    "c = SearchCrawler(['如何掌控你的自由时间', '阅读全世界', '如何做得更好'])\n",
    "c.run()\n",
    "print(c.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四、编程实践：使用爬虫框架获取上市公司利润数据\n",
    "- http://quote.eastmoney.com/newapi/getlrqs?code=600019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
